<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CPSC_440 - Lecture 2026-01-05</title>
    <link rel="stylesheet" href="../../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
</head>
<body>
    <div class="container">
        <nav class="breadcrumb">
            <a href="../../index.html">Home</a> /
            <a href="../../index.html#cpsc440">CPSC_440</a> /
            <span>2026-01-05</span>
        </nav>

        <header class="lecture-header">
            <h1>Advanced Machine Learning</h1>
            <div class="lecture-meta">
                <span class="date">üìÖ 2026-01-05</span>
                <span class="instructor">üë§ Danica Sutherland</span>
            </div>
        </header>

        <div class="lecture-content">
            <section class="raw-notes">
                <h2>Quick Notes</h2>
                <div class="notes-box">
                    <p><strong>Grading:</strong></p>
                    <ul>
                        <li>40% assignments (4-ish). A1 alone, no late days. A2+ with partner or alone. 5 total late days over term</li>
                        <li>20% quizzes. Self-scheduled, about every other week, short conceptual questions. Need to do 5/6</li>
                        <li>40% max(projects, final exam)</li>
                    </ul>

                    <p><strong>Important links:</strong></p>
                    <ul>
                        <li>Course website: https://www.cs.ubc.ca/~dsuth/440/25w2/</li>
                        <li>Gradescope: https://www.gradescope.ca/courses/34664</li>
                        <li>Piazza: https://piazza.com/class/mk0uk8835r31ic</li>
                    </ul>

                    <p><strong>General problem: Binary density estimation</strong></p>
                    <ul>
                        <li>Special case of density estimation with binary data</li>
                        <li>Example: What is the probability that a randomly selected student has covid?</li>
                        <li>Input: n iid samples of binary values x¬π,x¬≤,...x‚Åø in {0,1}</li>
                        <li>Output: a probability model for random variable X: here, just Pr(X=1)</li>
                    </ul>

                    <p><strong>Model: Bernoulli distribution</strong></p>
                    <ul>
                        <li>x ~ Bern(Œ∏) if Pr(x=1|Œ∏) = Œ∏</li>
                        <li>p(x|Œ∏) = Œ∏À£(1-Œ∏)¬π‚ÅªÀ£</li>
                        <li>Inference: Given Œ∏, compute p(x¬π,x¬≤,...x‚Åø|Œ∏)</li>
                    </ul>

                    <p><strong>ML Terminology:</strong></p>
                    <ul>
                        <li>Learning: Going from data to parameters Œ∏</li>
                        <li>Inference: Using parameters Œ∏ to infer/predict something</li>
                    </ul>

                    <p><strong>Computing in log space:</strong></p>
                    <pre><code class="language-python">n_1 = X.sum()
n_0 = X.shape[0] - n_1
log_p = n_1 * np.log(theta) + n_0 * np.log1p(-theta)</code></pre>
                </div>
            </section>

            <section class="expanded-notes">
                <h2>Detailed Notes</h2>
                <div id="notes-content">

                    <!-- ===== COURSE LOGISTICS ===== -->
                    <article>
                        <h3>Course Structure and Grading</h3>

                        <p>CPSC 440 takes a balanced approach to assessment, emphasizing both practical implementation skills and conceptual understanding. The grading structure is designed to give you flexibility while maintaining academic rigor.</p>

                        <h4>Grading Breakdown</h4>

                        <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                            <thead>
                                <tr style="background: var(--secondary-bg);">
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">Component</th>
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">Details</th>
                                    <th style="padding: 0.75rem; text-align: right; border: 1px solid var(--border-color);">Weight</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><strong>Assignments</strong></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">~4 assignments total</td>
                                    <td style="padding: 0.75rem; text-align: right; border: 1px solid var(--border-color);">40%</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><strong>Quizzes</strong></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">6 quizzes, must complete 5</td>
                                    <td style="padding: 0.75rem; text-align: right; border: 1px solid var(--border-color);">20%</td>
                                </tr>
                                <tr style="background: var(--secondary-bg);">
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><strong>Project/Final Exam</strong></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Whichever is higher</td>
                                    <td style="padding: 0.75rem; text-align: right; border: 1px solid var(--border-color);"><strong>40%</strong></td>
                                </tr>
                            </tbody>
                        </table>

                        <h4>Assignment Policy Details</h4>

                        <p><strong>Assignment 1 (A1):</strong></p>
                        <ul>
                            <li>Must be completed <strong>individually</strong></li>
                            <li>No late days allowed</li>
                            <li>This ensures everyone gets hands-on experience with the fundamentals before partnering up</li>
                        </ul>

                        <p><strong>Assignments 2+ (A2, A3, A4):</strong></p>
                        <ul>
                            <li>Can be completed <strong>alone or with a partner</strong></li>
                            <li>You have <strong>5 total late days</strong> across the entire term</li>
                            <li>Late days can be used flexibly across assignments (e.g., 2 days on A2, 3 days on A3)</li>
                            <li>Working with a partner can help with workload but requires coordination and communication</li>
                        </ul>

                        <h4>Quiz Structure</h4>

                        <p>The quiz system is designed to be flexible and low-stress:</p>
                        <ul>
                            <li><strong>Self-scheduled</strong>: You choose when to take them within a given window</li>
                            <li><strong>Frequency</strong>: Approximately every other week (6 total throughout the term)</li>
                            <li><strong>Content</strong>: Short conceptual questions testing understanding, not calculation speed</li>
                            <li><strong>Requirement</strong>: Complete 5 out of 6 quizzes (you can skip one without penalty)</li>
                            <li><strong>Purpose</strong>: Keep you engaged with the material regularly and provide low-stakes checkpoints</li>
                        </ul>

                        <h4>Project vs. Final Exam</h4>

                        <p>The course offers flexibility for the final 40% of your grade:</p>

                        <blockquote>
                            <p><strong>Your grade will be max(project_grade, final_exam_grade)</strong></p>
                        </blockquote>

                        <p><strong>What this means:</strong></p>
                        <ul>
                            <li>You can choose to do a project, take the final exam, or <em>do both</em></li>
                            <li>If you do both, only the higher grade counts</li>
                            <li>The project allows you to explore a topic in depth and build something substantial</li>
                            <li>The final exam tests breadth of knowledge across all course topics</li>
                            <li>This structure lets you play to your strengths: hands-on implementation vs. comprehensive exam performance</li>
                        </ul>

                        <h4>Important Course Resources</h4>

                        <ul>
                            <li><strong>Course Website</strong>: <a href="https://www.cs.ubc.ca/~dsuth/440/25w2/" target="_blank">https://www.cs.ubc.ca/~dsuth/440/25w2/</a> ‚Äî Syllabus, lecture notes, schedule</li>
                            <li><strong>Gradescope</strong>: <a href="https://www.gradescope.ca/courses/34664" target="_blank">https://www.gradescope.ca/courses/34664</a> ‚Äî Submit assignments, view grades</li>
                            <li><strong>Piazza</strong>: <a href="https://piazza.com/class/mk0uk8835r31ic" target="_blank">https://piazza.com/class/mk0uk8835r31ic</a> ‚Äî Ask questions, discuss concepts, get help</li>
                        </ul>

                        <aside style="background: var(--secondary-bg); padding: 1.5rem; border-left: 4px solid var(--accent-color); margin: 2rem 0;">
                            <h5 style="margin-top: 0;">üí° Strategy Tip</h5>
                            <p>The flexible grading structure rewards consistent effort (assignments + quizzes = 60%) while giving you a choice in how to tackle the final 40%. Start thinking early about whether you want to pursue a project topic that excites you or prepare for a comprehensive exam.</p>
                        </aside>
                    </article>

                    <!-- ===== INTRODUCTION TO PROBABILISTIC MODELS ===== -->
                    <article>
                        <h3>Introduction: Why Binary Density Estimation?</h3>

                        <p>This course starts with the <strong>simplest possible machine learning problem</strong>: binary density estimation. While this might seem trivial, it's actually the perfect foundation for understanding the key concepts that underpin all of machine learning.</p>

                        <p><strong>The pedagogical strategy:</strong> We'll start with this very simple case and gradually build complexity, extending each concept to more sophisticated scenarios. By the end of the course, you'll see how the same principles scale to state-of-the-art models.</p>

                        <h4>The Binary Density Estimation Problem</h4>

                        <blockquote>
                            <p><strong>Problem Statement:</strong> Given a collection of binary (0 or 1) observations, estimate the probability that a randomly selected observation is 1.</p>
                        </blockquote>

                        <p><strong>Formal Setup:</strong></p>
                        <ul>
                            <li><strong>Input</strong>: <code>n</code> independent and identically distributed (i.i.d.) samples: <code>x¬π, x¬≤, ..., x‚Åø</code> where each <code>x‚Å± ‚àà {0, 1}</code></li>
                            <li><strong>Output</strong>: A probability model for a random variable <code>X</code>, specifically <code>Pr(X = 1)</code></li>
                            <li><strong>Data representation</strong>: <code>X ‚àà ‚Ñù‚ÅøÀ£¬π</code> is a column vector containing our <code>n</code> sample data points</li>
                        </ul>

                        <p><strong>Concrete Example:</strong></p>
                        <pre><code class="language-python">X = [1, 0, 0, 1, 0]  # 5 binary observations
# Our density estimator would output: Pr(X = 1) = 2/5 = 0.4</code></pre>

                        <p>We have 5 observations, 2 of which are 1's. The natural estimate is that the probability of observing a 1 is 0.4 or 40%.</p>

                        <h4>Real-World Applications</h4>

                        <p>Despite its simplicity, binary density estimation appears in many practical scenarios:</p>

                        <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                            <thead>
                                <tr style="background: var(--secondary-bg);">
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">Question</th>
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">What We're Estimating</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">What is the probability a randomly selected student has COVID?</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Pr(student has COVID) based on test results (1 = positive, 0 = negative)</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">What's the probability this medical treatment works?</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Pr(treatment success) from clinical trial data (1 = success, 0 = failure)</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">What's the probability that if you plant 10 seeds, at least one will germinate?</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">First estimate Pr(single seed germinates), then use it to compute more complex probabilities</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">How many lottery tickets should you expect to buy before you win?</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Estimate Pr(ticket wins), then compute expected number of trials</td>
                                </tr>
                            </tbody>
                        </table>

                        <p><strong>Key observation:</strong> In the first two examples, we directly compute <code>Pr(X = 1)</code>. In the last two examples, we use the estimated probability to <em>derive</em> other quantities of interest. All of these tasks fall under the umbrella of <strong>"inference"</strong> with the model.</p>

                        <h4>Beyond Binary Data</h4>

                        <p>While we start with binary data, the concepts we develop here generalize to:</p>
                        <ul>
                            <li><strong>Categorical data</strong>: More than two discrete outcomes (e.g., classifying images into 1000 categories)</li>
                            <li><strong>Continuous data</strong>: Real-valued observations (e.g., predicting house prices, temperature)</li>
                            <li><strong>Structured data</strong>: Sequences, images, graphs (e.g., language models, computer vision)</li>
                            <li><strong>High-dimensional data</strong>: Many variables observed simultaneously</li>
                        </ul>

                        <p>Each of these builds on the same fundamental ideas we're establishing with binary data.</p>
                    </article>

                    <!-- ===== ML TERMINOLOGY ===== -->
                    <article>
                        <h3>Machine Learning Terminology: Learning vs. Inference</h3>

                        <p>Before diving deeper, it's crucial to establish clear terminology. Unfortunately, different communities (machine learning vs. statistics) sometimes use the same words to mean different things.</p>

                        <h4>Machine Learning Terminology (What We'll Use)</h4>

                        <p><strong>Learning:</strong></p>
                        <blockquote>
                            <p>The process of going from <strong>data</strong> to <strong>parameters (Œ∏)</strong></p>
                        </blockquote>

                        <p>In other words, learning means:</p>
                        <ul>
                            <li>Observing data <code>X = [x¬π, x¬≤, ..., x‚Åø]</code></li>
                            <li>Using that data to estimate the parameters <code>Œ∏</code> of your model</li>
                            <li>Example: From the data <code>[1, 0, 0, 1, 0]</code>, learn that <code>Œ∏ = 0.4</code></li>
                        </ul>

                        <p><strong>Inference:</strong></p>
                        <blockquote>
                            <p>The task of using learned parameters <strong>Œ∏</strong> to <strong>infer/predict</strong> something</p>
                        </blockquote>

                        <p>Inference encompasses tasks like:</p>
                        <ul>
                            <li>Predicting the probability of a new observation: <code>Pr(X_new = 1 | Œ∏)</code></li>
                            <li>Computing probabilities for complex events: <code>Pr(at least 1 of 10 seeds germinates | Œ∏)</code></li>
                            <li>Evaluating how well the model explains observed data (likelihood)</li>
                            <li>Generating new samples from the distribution</li>
                        </ul>

                        <h4>Statistical Terminology (Different!)</h4>

                        <p>‚ö†Ô∏è <strong>Warning:</strong> Statisticians sometimes use "inference" to mean what machine learning calls "learning"‚Äîthat is, inferring parameters from data. This can cause confusion when reading papers or textbooks from different fields.</p>

                        <p><strong>In this course, we'll strictly use ML terminology:</strong></p>
                        <ul>
                            <li>Data ‚Üí Œ∏ = <strong>Learning</strong></li>
                            <li>Œ∏ ‚Üí Predictions/Probabilities = <strong>Inference</strong></li>
                        </ul>

                        <aside style="background: var(--secondary-bg); padding: 1.5rem; border-left: 4px solid var(--accent-color); margin: 2rem 0;">
                            <h5 style="margin-top: 0;">üéØ Mental Model</h5>
                            <p>Think of it as a two-stage process:</p>
                            <ol>
                                <li><strong>Learning (training time)</strong>: Observe data, estimate Œ∏</li>
                                <li><strong>Inference (test time)</strong>: Use Œ∏ to make predictions or answer questions</li>
                            </ol>
                            <p>This distinction maps cleanly to how we actually deploy ML systems: train once, infer many times.</p>
                        </aside>
                    </article>

                    <!-- ===== BERNOULLI DISTRIBUTION ===== -->
                    <article>
                        <h3>The Bernoulli Distribution: A Parameterized Probability Model</h3>

                        <p>To solve the binary density estimation problem, we need a <strong>parameterized probability model</strong>‚Äîthat is, a model with parameters we can learn from data. For binary variables, the standard choice is the <strong>Bernoulli distribution</strong>.</p>

                        <h4>Definition</h4>

                        <blockquote>
                            <p>A random variable <code>X</code> follows a <strong>Bernoulli distribution</strong> with parameter <code>Œ∏</code>, written <code>X ~ Bern(Œ∏)</code>, if:</p>
                            <p style="text-align: center; font-size: 1.1em;"><code>Pr(X = 1 | Œ∏) = Œ∏</code></p>
                        </blockquote>

                        <p><strong>What this means:</strong></p>
                        <ul>
                            <li><code>Œ∏</code> is the probability that <code>X = 1</code></li>
                            <li><code>Œ∏</code> must be between 0 and 1: <code>Œ∏ ‚àà [0, 1]</code></li>
                            <li>If <code>Œ∏ = 0.08</code>, then there's an 8% chance that <code>X = 1</code> (and a 92% chance that <code>X = 0</code>)</li>
                        </ul>

                        <p><strong>Example (COVID prevalence):</strong></p>
                        <p>If we model whether a student has COVID as <code>X ~ Bern(0.08)</code>, we're saying:</p>
                        <ul>
                            <li>8% of the student population has COVID (<code>Pr(X = 1) = 0.08</code>)</li>
                            <li>92% does not have COVID (<code>Pr(X = 0) = 0.92</code>)</li>
                        </ul>

                        <h4>Computing Pr(X = 0) from Œ∏</h4>

                        <p>Since probabilities must sum to 1, we can derive <code>Pr(X = 0 | Œ∏)</code> from <code>Pr(X = 1 | Œ∏)</code>:</p>

                        <pre><code class="language-plaintext">Pr(X = 0 | Œ∏) + Pr(X = 1 | Œ∏) = 1
Pr(X = 0 | Œ∏) + Œ∏ = 1
Pr(X = 0 | Œ∏) = 1 - Œ∏</code></pre>

                        <p><strong>Key insight:</strong> The Bernoulli distribution is fully specified by a single parameter <code>Œ∏</code>. Once you know <code>Œ∏</code>, you know everything about the distribution.</p>

                        <h4>Notation Variations</h4>

                        <p>You'll see the probability written in many different ways. They all mean the same thing:</p>

                        <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                            <thead>
                                <tr style="background: var(--secondary-bg);">
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">Notation</th>
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">What It Means</th>
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">When To Use</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><code>Pr(X = 0 | Œ∏)</code></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Probability that X equals 0, given parameter Œ∏</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Most explicit, clearest for beginners</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><code>p(0 | Œ∏)</code></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Same as above, lowercase p</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Common shorthand, saves writing</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><code>p_Œ∏(0)</code></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Probability of 0 under distribution parameterized by Œ∏</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">When comparing multiple models with different Œ∏</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><code>p(0)</code></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Probability of 0 (Œ∏ is implicit)</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">When Œ∏ is clear from context</td>
                                </tr>
                            </tbody>
                        </table>

                        <p>‚ö†Ô∏è <strong>Important:</strong> Be careful to understand what's being abbreviated. As the Python Zen says: <em>"Explicit is better than implicit."</em> When in doubt, use the most explicit notation to avoid confusion.</p>

                        <h4>Unified Formula for the Bernoulli Distribution</h4>

                        <p>It's sometimes helpful to express the Bernoulli distribution as a single formula that works for both <code>x = 0</code> and <code>x = 1</code>:</p>

                        <blockquote>
                            <p style="text-align: center; font-size: 1.1em;"><code>p(x | Œ∏) = Œ∏À£ (1 - Œ∏)¬π‚ÅªÀ£</code></p>
                        </blockquote>

                        <p><strong>Why this works:</strong></p>
                        <ul>
                            <li><strong>When x = 1:</strong> <code>p(1 | Œ∏) = Œ∏¬π (1 - Œ∏)‚Å∞ = Œ∏ ¬∑ 1 = Œ∏</code> ‚úì</li>
                            <li><strong>When x = 0:</strong> <code>p(0 | Œ∏) = Œ∏‚Å∞ (1 - Œ∏)¬π = 1 ¬∑ (1 - Œ∏) = 1 - Œ∏</code> ‚úì</li>
                        </ul>

                        <p><strong>Alternative notation using the indicator function:</strong></p>
                        <p style="text-align: center;"><code>p(x | Œ∏) = Œ∏^ùüô(x=1) (1 - Œ∏)^ùüô(x=0)</code></p>

                        <p>Where <code>ùüô(E)</code> is the <strong>indicator function</strong>:</p>
                        <ul>
                            <li><code>ùüô(E) = 1</code> if condition <code>E</code> is true</li>
                            <li><code>ùüô(E) = 0</code> if condition <code>E</code> is false</li>
                        </ul>

                        <p>This unified formula is elegant and useful for mathematical derivations, especially when we need to take derivatives or work with log probabilities.</p>

                        <aside style="background: var(--secondary-bg); padding: 1.5rem; border-left: 4px solid var(--accent-color); margin: 2rem 0;">
                            <h5 style="margin-top: 0;">üí° Intuition Check</h5>
                            <p>The formula <code>p(x | Œ∏) = Œ∏À£(1-Œ∏)¬π‚ÅªÀ£</code> might look complicated, but it's just a clever way to write:</p>
                            <ul>
                                <li>"If x is 1, the probability is Œ∏"</li>
                                <li>"If x is 0, the probability is 1-Œ∏"</li>
                            </ul>
                            <p>The exponents automatically "select" the right probability based on the value of x!</p>
                        </aside>
                    </article>

                    <!-- ===== INFERENCE TASKS ===== -->
                    <article>
                        <h3>Bernoulli Inference Tasks: Computing the Likelihood</h3>

                        <p>Now that we have a model (the Bernoulli distribution), we can perform <strong>inference</strong>‚Äîusing the parameters to compute probabilities and answer questions.</p>

                        <h4>The Likelihood Function</h4>

                        <p>One of the most important inference tasks is computing the <strong>likelihood</strong> of observing your dataset given the parameters.</p>

                        <blockquote>
                            <p><strong>Likelihood:</strong> Given parameter Œ∏ and observed data x¬π, x¬≤, ..., x‚Åø, compute the probability of observing exactly that data:</p>
                            <p style="text-align: center;"><code>p(x¬π, x¬≤, ..., x‚Åø | Œ∏)</code></p>
                        </blockquote>

                        <p><strong>Why is this important?</strong></p>
                        <ul>
                            <li><strong>Learning Œ∏</strong>: Maximum likelihood estimation (MLE) finds the Œ∏ that maximizes this probability</li>
                            <li><strong>Model comparison</strong>: Compare how well different models explain the data on validation/test sets</li>
                            <li><strong>Model selection</strong>: Choose between competing hypotheses</li>
                        </ul>

                        <h4>Independence Assumption</h4>

                        <p>If we assume that <code>X¬π, X¬≤, ..., X‚Åø</code> are <strong>independent given Œ∏</strong> (which is what "i.i.d." means‚Äîindependent and identically distributed), then:</p>

                        <blockquote>
                            <p style="text-align: center; font-size: 1.1em;"><code>p(x¬π, x¬≤, ..., x‚Åø | Œ∏) = ‚àè·µ¢‚Çå‚ÇÅ‚Åø p(x‚Å± | Œ∏)</code></p>
                        </blockquote>

                        <p><strong>In words:</strong> The probability of the entire dataset is the <em>product</em> of the probabilities of each individual observation.</p>

                        <p><strong>Why independence matters:</strong> Without this assumption, we'd need to model complex dependencies between observations, making the problem much harder. The i.i.d. assumption is common in ML because it simplifies both the math and the algorithms.</p>

                        <h4>Example Calculation</h4>

                        <p>Let's compute the likelihood for a concrete example:</p>

                        <p><strong>Data:</strong> <code>X = [1, 0, 1, 1]</code><br>
                        <strong>Parameter:</strong> <code>Œ∏ = 0.6</code></p>

                        <pre><code class="language-plaintext">p(X | Œ∏) = p(1 | Œ∏) ¬∑ p(0 | Œ∏) ¬∑ p(1 | Œ∏) ¬∑ p(1 | Œ∏)
         = Œ∏ ¬∑ (1 - Œ∏) ¬∑ Œ∏ ¬∑ Œ∏
         = 0.6 ¬∑ 0.4 ¬∑ 0.6 ¬∑ 0.6
         = 0.6¬≥ ¬∑ 0.4¬π
         = 0.216 ¬∑ 0.4
         = 0.0864</code></pre>

                        <p>So there's an 8.64% probability of observing exactly this sequence of data if <code>Œ∏ = 0.6</code>.</p>

                        <h4>Simplified Formula Using Counts</h4>

                        <p>Notice that in the example above, we ended up with <code>Œ∏¬≥ ¬∑ (1-Œ∏)¬π</code>. This pattern generalizes:</p>

                        <blockquote>
                            <p>If your dataset has <code>n‚ÇÅ</code> ones and <code>n‚ÇÄ</code> zeros, then:</p>
                            <p style="text-align: center; font-size: 1.1em;"><code>p(X | Œ∏) = Œ∏^(n‚ÇÅ) ¬∑ (1 - Œ∏)^(n‚ÇÄ)</code></p>
                        </blockquote>

                        <p>Where:</p>
                        <ul>
                            <li><code>n‚ÇÅ = number of 1's in the dataset</code></li>
                            <li><code>n‚ÇÄ = number of 0's in the dataset</code></li>
                            <li><code>n‚ÇÅ + n‚ÇÄ = n</code> (total number of samples)</li>
                        </ul>

                        <p><strong>Key insight:</strong> The order of observations doesn't matter for computing likelihood‚Äîonly the counts!</p>

                        <p>Using indicator functions, we can also write:</p>
                        <p style="text-align: center;"><code>p(X | Œ∏) = Œ∏^(Œ£·µ¢ ùüô(x‚Å±=1)) ¬∑ (1-Œ∏)^(Œ£·µ¢ ùüô(x‚Å±=0))</code></p>

                        <p>The summations just count how many times each value appears in the dataset.</p>

                        <aside style="background: var(--secondary-bg); padding: 1.5rem; border-left: 4px solid var(--accent-color); margin: 2rem 0;">
                            <h5 style="margin-top: 0;">üîç Why Use Counts?</h5>
                            <p>Computing the likelihood by multiplying <code>n</code> individual probabilities is inefficient. By recognizing that only the <em>counts</em> matter, we reduce <code>O(n)</code> multiplications to just <code>O(n)</code> additions (to count), plus two exponentiations. This is much faster and more stable numerically.</p>
                        </aside>
                    </article>

                    <!-- ===== COMPUTING IN LOG SPACE ===== -->
                    <article>
                        <h3>Computing Probabilities in Log Space: A Practical Necessity</h3>

                        <p>In practice, we almost <strong>never</strong> compute probabilities directly. Instead, we work in <strong>log space</strong>. This isn't just a mathematical trick‚Äîit's essential for numerical stability and computational efficiency.</p>

                        <h4>The Problem with Direct Probability Computation</h4>

                        <p>Consider computing the likelihood for a large dataset:</p>
                        <ul>
                            <li>If <code>n = 10,000</code> and <code>Œ∏ = 0.5</code>, then <code>p(X | Œ∏) ‚âà 0.5^10000</code></li>
                            <li>This number is approximately <code>10^(-3010)</code></li>
                            <li>Standard floating-point numbers (64-bit doubles) can only represent values down to about <code>10^(-308)</code></li>
                            <li><strong>Result: Numerical underflow</strong>‚Äîthe computer rounds the probability to exactly 0</li>
                        </ul>

                        <p>Once you hit 0, you've lost all information. You can't compare models, you can't compute gradients for learning, and optimization breaks down.</p>

                        <h4>The Solution: Work in Log Space</h4>

                        <p>Instead of computing <code>p(X | Œ∏)</code>, compute <code>log p(X | Œ∏)</code>:</p>

                        <pre><code class="language-plaintext">log p(X | Œ∏) = log(Œ∏^(n‚ÇÅ) ¬∑ (1-Œ∏)^(n‚ÇÄ))
             = log(Œ∏^(n‚ÇÅ)) + log((1-Œ∏)^(n‚ÇÄ))     [log(ab) = log(a) + log(b)]
             = n‚ÇÅ ¬∑ log(Œ∏) + n‚ÇÄ ¬∑ log(1-Œ∏)       [log(a^b) = b¬∑log(a)]</code></pre>

                        <p><strong>Benefits of log space:</strong></p>
                        <ul>
                            <li><strong>Numerical stability</strong>: <code>log(0.5^10000) = 10000 ¬∑ log(0.5) ‚âà -6931</code>‚Äîwell within floating-point range!</li>
                            <li><strong>No underflow</strong>: Logs of tiny probabilities are just large negative numbers</li>
                            <li><strong>Computational efficiency</strong>: Multiplication becomes addition (faster)</li>
                            <li><strong>Better accuracy</strong>: Floating-point arithmetic is more accurate for addition than multiplication</li>
                        </ul>

                        <h4>Python Implementation</h4>

                        <p>Here's the efficient way to compute log-likelihood in Python:</p>

                        <pre><code class="language-python">import numpy as np

def compute_log_likelihood(X, theta):
    """
    Compute log p(X | theta) for Bernoulli data.

    Parameters:
    -----------
    X : array-like of shape (n,)
        Binary observations (0s and 1s)
    theta : float
        Bernoulli parameter (probability of 1)

    Returns:
    --------
    log_p : float
        Log probability of the data
    """
    n_1 = X.sum()                    # Count the 1's
    n_0 = X.shape[0] - n_1          # Count the 0's

    # Compute log-likelihood
    log_p = n_1 * np.log(theta) + n_0 * np.log1p(-theta)

    return log_p</code></pre>

                        <p><strong>Key implementation details:</strong></p>

                        <ol>
                            <li><strong>Counting:</strong> <code>X.sum()</code> efficiently counts the 1's (since 0's contribute nothing to the sum)</li>
                            <li><strong>Subtraction:</strong> <code>n_0 = X.shape[0] - n_1</code> gives us the count of 0's</li>
                            <li><strong>Log computation:</strong> <code>np.log(theta)</code> for the 1's term</li>
                            <li><strong>Special function:</strong> <code>np.log1p(-theta)</code> instead of <code>np.log(1 - theta)</code></li>
                        </ol>

                        <h4>Why np.log1p?</h4>

                        <p>The function <code>np.log1p(t)</code> computes <code>log(1 + t)</code> with much better numerical precision when <code>t</code> is close to 0.</p>

                        <p><strong>The problem:</strong></p>
                        <ul>
                            <li>Floating-point numbers are more accurate near 0 than near 1</li>
                            <li>If <code>theta = 0.9999</code>, then <code>1 - theta = 0.0001</code></li>
                            <li>Computing <code>1 - 0.9999</code> in floating-point can lose precision</li>
                            <li>Then taking <code>log</code> amplifies that error</li>
                        </ul>

                        <p><strong>The solution:</strong></p>
                        <ul>
                            <li><code>np.log1p(-theta)</code> is mathematically equal to <code>np.log(1 - theta)</code></li>
                            <li>But it computes the result using a specialized algorithm that's accurate even when <code>theta ‚âà 1</code></li>
                            <li>This is a standard numerical computing best practice</li>
                        </ul>

                        <h4>Computational Complexity</h4>

                        <p>Let's analyze the algorithm's efficiency:</p>

                        <ol>
                            <li><strong>Counting:</strong> <code>X.sum()</code> scans through all <code>n</code> elements ‚Üí <code>O(n)</code></li>
                            <li><strong>Arithmetic:</strong> Subtraction, multiplication, addition ‚Üí <code>O(1)</code></li>
                            <li><strong>Logarithms:</strong> Two <code>log</code> operations ‚Üí <code>O(1)</code></li>
                        </ol>

                        <p><strong>Total complexity:</strong> <code>O(n)</code> time, <code>O(1)</code> additional space</p>

                        <p>This is optimal‚Äîwe must look at each data point at least once to compute the likelihood!</p>

                        <aside style="background: var(--secondary-bg); padding: 1.5rem; border-left: 4px solid var(--accent-color); margin: 2rem 0;">
                            <h5 style="margin-top: 0;">‚ö° Performance Tip</h5>
                            <p><strong>Always work in log space when doing probabilistic machine learning.</strong></p>
                            <ul>
                                <li>It prevents numerical underflow</li>
                                <li>It's faster (addition vs multiplication)</li>
                                <li>It's more accurate (better floating-point behavior)</li>
                                <li>Optimization algorithms work better with log-likelihood</li>
                            </ul>
                            <p>Most ML libraries (PyTorch, TensorFlow, JAX) provide log-space versions of probability functions precisely for this reason.</p>
                        </aside>
                    </article>

                </div>
            </section>

            <section class="topics">
                <h2>Topics Covered</h2>
                <ul id="topics-list">
                    <li>Understanding course structure: flexible grading with assignments, quizzes, and project/exam choice</li>
                    <li>Introduction to binary density estimation as the foundation for machine learning</li>
                    <li>Clarifying ML terminology: Learning (data ‚Üí Œ∏) vs. Inference (Œ∏ ‚Üí predictions)</li>
                    <li>Defining the Bernoulli distribution and its parameterization</li>
                    <li>Computing likelihood for i.i.d. data using the independence assumption</li>
                    <li>Working in log space for numerical stability and computational efficiency</li>
                    <li>Implementing efficient log-likelihood computation in Python with np.log1p</li>
                </ul>
            </section>

            <section class="assignments">
                <h2>Assignments & Action Items</h2>
                <ul id="assignments-list">
                    <li>Review course website, Gradescope, and Piazza‚Äîfamiliarize yourself with all resources</li>
                    <li>Understand the difference between Learning and Inference (this distinction is fundamental!)</li>
                    <li>Practice deriving Pr(X=0|Œ∏) = 1-Œ∏ from the probability axioms</li>
                    <li>Work through the unified Bernoulli formula p(x|Œ∏) = Œ∏À£(1-Œ∏)¬π‚ÅªÀ£ for x=0 and x=1</li>
                    <li>Verify the likelihood calculation example with X=[1,0,1,1] and Œ∏=0.6</li>
                    <li>Implement the compute_log_likelihood function and test it with sample data</li>
                    <li>Understand why np.log1p(-theta) is better than np.log(1-theta)</li>
                </ul>
            </section>
        </div>

        <footer class="lecture-footer">
            <a href="../../index.html#cpsc440" class="back-link">‚Üê Back to CPSC_440</a>
        </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
